{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated performance specialization experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the target system name. Systems available : Apache, BerkeleyC, BerkeleyJ, Dune, HIPAcc, HSMGP, LLVM, SQLite, Linux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_name = \"Linux\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy\n",
    "\n",
    "def get_dataset(name):\n",
    "    if name == \"Linux\":\n",
    "        with open(\"datasets/Linux_options.json\",\"r\") as f:\n",
    "            linux_options = json.load(f)\n",
    "        # Load csv by setting options as int8 to save a lot of memory\n",
    "        return pd.read_csv(\"datasets/Linux.csv\", dtype={f:numpy.int8 for f in linux_options})\n",
    "    else :\n",
    "        return pd.read_csv(\"datasets/{}.csv\".format(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_dataset(system_name)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the experimental exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimental parameters\n",
    "## Thresholds, or performance objectives, are defined according to the dataset performance distribution\n",
    "thresholds = [df[\"perf\"].quantile(i) for i in [0.1,0.2,0.5,0.8,0.9]]\n",
    "## Size of the training set, up to 70% to keep at least 30% of the dataset as testing set\n",
    "training_sizes = [0.1,0.2,0.5,0.7]\n",
    "## Number of times each training is repeated. You might want to reduce it to 5 for Linux as it would be very long and not that useful as it is quite stable\n",
    "n_repeats = 20\n",
    "## Parameter only for spcialized regression, the gap to be created in the performance distribution at the threshold level\n",
    "gaps = [df[\"perf\"].max(), df[\"perf\"].mean(), df[\"perf\"].mean()/2, df[\"perf\"].mean()/4]\n",
    "\n",
    "print(\"Thresholds : \", thresholds)\n",
    "print(\"Gaps : \", gaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypermarameters grid search values\n",
    "classification_criterion = [\"gini\",\"entropy\"]\n",
    "regression_criterion = [\"mse\",\"friedman_mse\"]\n",
    "\n",
    "max_depths = [5,8,10,12,14,16,18,20]\n",
    "min_samples_splits = [2,5,10,20,50,100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing usefull functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import ensemble, tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def mape(y_true, y_pred):\n",
    "    \"\"\"Return the Mean Absolute Percentage Error\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
    "        Ground truth (correct) target values.\n",
    "    y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
    "        Estimated target values.\n",
    "    \"\"\"\n",
    "    return pd.Series(((y_pred - y_true) / y_true).abs() * 100).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classification_tree(hp, threshold, train_size):\n",
    "    accuracy = []\n",
    "    # Repeat the learning a set number of time\n",
    "    for _ in range(n_repeats):\n",
    "        # Split the dataset\n",
    "        X_train, X_test, y_train, y_test = train_test_split(df.drop(columns=[\"perf\"]), df[\"perf\"] > threshold, train_size=train_size)\n",
    "        \n",
    "        # Set Decision Tree hyperparameters\n",
    "        clf = tree.DecisionTreeClassifier(**hp)\n",
    "        # Train it\n",
    "        clf.fit(X_train, y_train)\n",
    "        \n",
    "        # Save the balanced accuracy\n",
    "        accuracy.append(balanced_accuracy_score(\n",
    "            y_test,\n",
    "            clf.predict(X_test)\n",
    "        ))\n",
    "    acc = pd.Series(accuracy)\n",
    "    # Report the average accuracy and the standard deviation\n",
    "    return acc.mean(), acc.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification = []\n",
    "\n",
    "# For each combination of parameters and hyperparameters, perform the experiment\n",
    "for max_depth in max_depths:\n",
    "    for min_samples_split in min_samples_splits:\n",
    "        for criterion in classification_criterion:\n",
    "            for training_size in training_sizes:\n",
    "                for threshold in thresholds:\n",
    "                    hp = {\n",
    "                        \"max_depth\":max_depth,\n",
    "                        \"min_samples_split\":min_samples_split,\n",
    "                        \"criterion\":criterion\n",
    "                    }\n",
    "                    mean, std = train_classification_tree(hp, threshold, training_size)\n",
    "                    hp[\"threshold\"] = threshold\n",
    "                    hp[\"training_size\"] = training_size\n",
    "                    hp[\"mean\"] = mean\n",
    "                    hp[\"std\"] = std\n",
    "                    classification.append(hp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data into a DataFrame\n",
    "df_classification = pd.DataFrame(classification)\n",
    "# Display the best results for each parameters\n",
    "df_classification.groupby([\"threshold\",\"training_size\"])[\"mean\"].max().unstack('threshold')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_regression_tree(hp, thresholds, train_size):\n",
    "    accuracy = {t:[] for t in thresholds}\n",
    "    for _ in range(n_repeats):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(df.drop(columns=[\"perf\"]), df[\"perf\"], train_size=train_size)\n",
    "        \n",
    "        clf = tree.DecisionTreeRegressor(**hp)\n",
    "        clf.fit(X_train, y_train)\n",
    "        \n",
    "        # Save the balanced accuracy for each threshold, as the regression model is threshold agnostic\n",
    "        for threshold in thresholds:\n",
    "            accuracy[threshold].append(balanced_accuracy_score(\n",
    "                y_test > threshold,\n",
    "                clf.predict(X_test) > threshold\n",
    "            ))\n",
    "    # Report average balanced accuracy and the stadard deviation indexed by threshold\n",
    "    return {t:{\"mean\":pd.Series(accuracy[threshold]).mean(),\"std\":pd.Series(accuracy[threshold]).std()} for t in thresholds}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression = []\n",
    "\n",
    "for max_depth in max_depths:\n",
    "    for min_samples_split in min_samples_splits:\n",
    "        for criterion in regression_criterion:\n",
    "            for training_size in training_sizes:\n",
    "                hp = {\n",
    "                    \"max_depth\":max_depth,\n",
    "                    \"min_samples_split\":min_samples_split,\n",
    "                    \"criterion\":criterion\n",
    "                }\n",
    "                for threshold, i in train_regression_tree(hp, thresholds, training_size).items():\n",
    "\n",
    "                    i[\"max_depth\"] = max_depth\n",
    "                    i[\"min_samples_split\"] = min_samples_split\n",
    "                    i[\"criterion\"] = criterion\n",
    "                    i[\"threshold\"] = threshold\n",
    "                    i[\"training_size\"] = training_size\n",
    "                    regression.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_regression = pd.DataFrame(regression)\n",
    "df_regression.groupby([\"threshold\",\"training_size\"])[\"mean\"].max().unstack('threshold')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specialized Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_specialized_regression_tree(hp, threshold, train_size, gap):\n",
    "    accuracy = []\n",
    "    for _ in range(n_repeats):\n",
    "        # .map(lambda x : x + gap if x > threshold else x) <- this creates the gap\n",
    "        X_train, X_test, y_train, y_test = train_test_split(df.drop(columns=[\"perf\"]), df[\"perf\"].map(lambda x : x + gap if x > threshold else x), train_size=train_size)\n",
    "        \n",
    "        clf = tree.DecisionTreeRegressor(**hp)\n",
    "        clf.fit(X_train, y_train)\n",
    "        accuracy.append(balanced_accuracy_score(\n",
    "            y_test > threshold,\n",
    "            clf.predict(X_test) > threshold\n",
    "        ))\n",
    "    acc = pd.Series(accuracy)\n",
    "    return acc.mean(), acc.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specialized_regression = []\n",
    "\n",
    "for max_depth in max_depths:\n",
    "    for min_samples_split in min_samples_splits:\n",
    "        for criterion in regression_criterion:\n",
    "            for training_size in training_sizes:\n",
    "                for threshold in thresholds:\n",
    "                    for gap in gaps:\n",
    "                        hp = {\n",
    "                            \"max_depth\":max_depth,\n",
    "                            \"min_samples_split\":min_samples_split,\n",
    "                            \"criterion\":criterion\n",
    "                        }\n",
    "                        mean, std = train_specialized_regression_tree(hp, threshold, training_size, gap)\n",
    "                        hp[\"threshold\"] = threshold\n",
    "                        hp[\"training_size\"] = training_size\n",
    "                        hp[\"gap\"] = gap\n",
    "                        hp[\"mean\"] = mean\n",
    "                        hp[\"std\"] = std\n",
    "                        specialized_regression.append(hp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spec_regression = pd.DataFrame(specialized_regression)\n",
    "df_spec_regression.groupby([\"threshold\",\"training_size\"])[\"mean\"].max().unstack('threshold')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Ranking List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To know which features to select, we rely on a Feature Ranking List, an automatically synthesized list of features ranked by their importance according to Random Forests trained on a dataset from the target system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frl_models = []\n",
    "for _ in range(20):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df.drop(columns=[\"perf\"]), df[\"perf\"], train_size=0.1)\n",
    "    reg = ensemble.RandomForestRegressor(max_depth=12, min_samples_split=5, criterion=\"mse\", n_jobs=-1)\n",
    "    reg.fit(X_train, y_train)\n",
    "    y_pred = reg.predict(X_test)\n",
    "    acc = mape(\n",
    "        y_test,\n",
    "        reg.predict(X_test)\n",
    "    )\n",
    "\n",
    "    frl_models.append({\n",
    "        \"model\":reg,\n",
    "        \"error\":acc\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_importance = pd.DataFrame([i[\"model\"].feature_importances_ for i in frl_models], columns=X_train.columns)\n",
    "df_importance.loc[\"mean\"] = df_importance.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing an average ranking for each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_values = df_importance.T\n",
    "for i in df_values.columns:\n",
    "    df_values[\"ranking-\"+str(i)] = df_values[i].sort_values(ascending=False).rank(method=\"min\", ascending=False)\n",
    "df_values.sort_values(\"ranking-mean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the Feature Ranking List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_ranking_list = list(df_values.sort_values(\"ranking-mean\")[\"ranking-mean\"].index)\n",
    "feature_ranking_list[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of options to consider\n",
    "list_n_options = list(range(1,len(feature_ranking_list)))\n",
    "# Linux has too many options to consider all\n",
    "if system_name == \"Linux\":\n",
    "    list_n_options = [150,200,250,300,350,400,450,500,1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classification_tree_fs(hp, threshold, train_size, features):\n",
    "    accuracy = []\n",
    "    for _ in range(n_repeats):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(df[features], df[\"perf\"] > threshold, train_size=train_size)\n",
    "        \n",
    "        clf = tree.DecisionTreeClassifier(**hp)\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        accuracy.append(balanced_accuracy_score(\n",
    "            y_test,\n",
    "            clf.predict(X_test)\n",
    "        ))\n",
    "    acc = pd.Series(accuracy)\n",
    "    return acc.mean(), acc.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_fs = []\n",
    "\n",
    "for max_depth in max_depths:\n",
    "    for min_samples_split in min_samples_splits:\n",
    "        for criterion in classification_criterion:\n",
    "            for training_size in training_sizes:\n",
    "                for threshold in thresholds:\n",
    "                    for n_options in list_n_options:\n",
    "                        hp = {\n",
    "                            \"max_depth\":max_depth,\n",
    "                            \"min_samples_split\":min_samples_split,\n",
    "                            \"criterion\":criterion\n",
    "                        }\n",
    "                        mean, std = train_classification_tree_fs(hp, threshold, training_size, feature_ranking_list[:n_options])\n",
    "                        hp[\"threshold\"] = threshold\n",
    "                        hp[\"training_size\"] = training_size\n",
    "                        hp[\"n_options\"] = n_options\n",
    "                        hp[\"mean\"] = mean\n",
    "                        hp[\"std\"] = std\n",
    "                        classification_fs.append(hp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_classification_fs = pd.DataFrame(classification_fs)\n",
    "df_classification_fs.groupby([\"threshold\",\"training_size\"])[\"mean\"].max().unstack('threshold')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_regression_tree_fs(hp, thresholds, train_size, features):\n",
    "    accuracy = {t:[] for t in thresholds}\n",
    "    for _ in range(n_repeats):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(df[features], df[\"perf\"], train_size=train_size)\n",
    "        \n",
    "        clf = tree.DecisionTreeRegressor(**hp)\n",
    "        clf.fit(X_train, y_train)\n",
    "        \n",
    "        for threshold in thresholds:\n",
    "            accuracy[threshold].append(balanced_accuracy_score(\n",
    "                y_test > threshold,\n",
    "                clf.predict(X_test) > threshold\n",
    "            ))\n",
    "    return {t:{\"mean\":pd.Series(accuracy[threshold]).mean(),\"std\":pd.Series(accuracy[threshold]).std()} for t in thresholds}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_fs = []\n",
    "\n",
    "for max_depth in max_depths:\n",
    "    for min_samples_split in min_samples_splits:\n",
    "        for criterion in regression_criterion:\n",
    "            for training_size in training_sizes:\n",
    "                for n_options in list_n_options:\n",
    "                    hp = {\n",
    "                        \"max_depth\":max_depth,\n",
    "                        \"min_samples_split\":min_samples_split,\n",
    "                        \"criterion\":criterion\n",
    "                    }\n",
    "                    for threshold, i in train_regression_tree_fs(hp, thresholds, training_size, feature_ranking_list[:n_options]).items():\n",
    "\n",
    "                        i[\"max_depth\"] = max_depth\n",
    "                        i[\"min_samples_split\"] = min_samples_split\n",
    "                        i[\"criterion\"] = criterion\n",
    "                        i[\"threshold\"] = threshold\n",
    "                        i[\"training_size\"] = training_size\n",
    "                        i[\"n_options\"] = n_options\n",
    "                        regression_fs.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_regression_fs = pd.DataFrame(regression_fs)\n",
    "df_regression_fs.groupby([\"threshold\",\"training_size\"])[\"mean\"].max().unstack('threshold')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specialized Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_specialized_regression_tree_fs(hp, threshold, train_size, gap, features):\n",
    "    accuracy = []\n",
    "    for _ in range(n_repeats):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(df[features], df[\"perf\"].map(lambda x : x + gap if x > threshold else x), train_size=train_size)\n",
    "        \n",
    "        clf = tree.DecisionTreeRegressor(**hp)\n",
    "        clf.fit(X_train, y_train)\n",
    "        accuracy.append(balanced_accuracy_score(\n",
    "            y_test > threshold,\n",
    "            clf.predict(X_test) > threshold\n",
    "        ))\n",
    "    acc = pd.Series(accuracy)\n",
    "    return acc.mean(), acc.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specialized_regression_fs = []\n",
    "\n",
    "for max_depth in max_depths:\n",
    "    for min_samples_split in min_samples_splits:\n",
    "        for criterion in regression_criterion:\n",
    "            for training_size in training_sizes:\n",
    "                for threshold in thresholds:\n",
    "                    for gap in gaps:\n",
    "                        for n_options in list_n_options:\n",
    "                            hp = {\n",
    "                                \"max_depth\":max_depth,\n",
    "                                \"min_samples_split\":min_samples_split,\n",
    "                                \"criterion\":criterion\n",
    "                            }\n",
    "                            mean, std = train_specialized_regression_tree_fs(hp, threshold, training_size, gap, feature_ranking_list[:n_options])\n",
    "                            hp[\"threshold\"] = threshold\n",
    "                            hp[\"training_size\"] = training_size\n",
    "                            hp[\"gap\"] = gap\n",
    "                            hp[\"n_options\"] = n_options\n",
    "                            hp[\"mean\"] = mean\n",
    "                            hp[\"std\"] = std\n",
    "                            specialized_regression_fs.append(hp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spec_regression_fs = pd.DataFrame(specialized_regression_fs)\n",
    "df_spec_regression_fs.groupby([\"threshold\",\"training_size\"])[\"mean\"].max().unstack('threshold')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export to Latex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automating the table generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.concat([df_classification, df_classification_fs, df_regression, df_regression_fs, df_spec_regression, df_spec_regression_fs])\n",
    "\n",
    "s = system_name\n",
    "s += \" & \"\n",
    "s += \"{:.1f}\\\\%\".format(df_all.groupby([\"threshold\",\"training_size\"])[\"mean\"].max().unstack('threshold').loc[0.7].mean() * 100)\n",
    "s += r\" ($\\pm${:.1f})\".format(df_all.groupby([\"threshold\",\"training_size\"])[\"mean\"].max().unstack('threshold').loc[0.7].std() * 100)\n",
    "s += \" & \"\n",
    "s += \"{:.1f}\\\\%\".format(df_classification.groupby([\"threshold\",\"training_size\"])[\"mean\"].max().unstack('threshold').loc[0.7].mean() * 100)\n",
    "s += r\" ($\\pm${:.1f})\".format(df_classification.groupby([\"threshold\",\"training_size\"])[\"mean\"].max().unstack('threshold').loc[0.7].std() * 100)\n",
    "s += \" & \"\n",
    "s += \"{:.1f}\\\\%\".format(df_classification_fs.groupby([\"threshold\",\"training_size\"])[\"mean\"].max().unstack('threshold').loc[0.7].mean() * 100)\n",
    "s += r\" ($\\pm${:.1f})\".format(df_classification_fs.groupby([\"threshold\",\"training_size\"])[\"mean\"].max().unstack('threshold').loc[0.7].std() * 100)\n",
    "s += \" & \"\n",
    "s += \"{:.1f}\".format(df_regression.groupby([\"threshold\",\"training_size\"])[\"mean\"].max().unstack('threshold').loc[0.7].mean() * 100)\n",
    "s += r\" ($\\pm${:.1f})\".format(df_regression.groupby([\"threshold\",\"training_size\"])[\"mean\"].max().unstack('threshold').loc[0.7].std() * 100)\n",
    "s += \" & \"\n",
    "s += \"{:.1f}\\\\%\".format(df_regression_fs.groupby([\"threshold\",\"training_size\"])[\"mean\"].max().unstack('threshold').loc[0.7].mean() * 100)\n",
    "s += r\" ($\\pm${:.1f})\".format(df_regression_fs.groupby([\"threshold\",\"training_size\"])[\"mean\"].max().unstack('threshold').loc[0.7].std() * 100)\n",
    "s += \" & \"\n",
    "s += \"{:.1f}\\\\%\".format(df_spec_regression.groupby([\"threshold\",\"training_size\"])[\"mean\"].max().unstack('threshold').loc[0.7].mean() * 100)\n",
    "s += r\" ($\\pm${:.1f})\".format(df_spec_regression.groupby([\"threshold\",\"training_size\"])[\"mean\"].max().unstack('threshold').loc[0.7].std() * 100)\n",
    "s += \" & \"\n",
    "s += \"{:.1f}\\\\%\".format(df_spec_regression_fs.groupby([\"threshold\",\"training_size\"])[\"mean\"].max().unstack('threshold').loc[0.7].mean() * 100)\n",
    "s += r\" ($\\pm${:.1f})\".format(df_spec_regression_fs.groupby([\"threshold\",\"training_size\"])[\"mean\"].max().unstack('threshold').loc[0.7].std() * 100)\n",
    "\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per system table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\\\begin{table*}\")\n",
    "print(\"\\\\begin{tabular}{ |l|ccccc| }\")\n",
    "print(\"\\\\hline\")\n",
    "print(\"\\\\multirow{2}{*}{Training set size} & \\\\multicolumn{4}{c}{\\\\hspace{2cm}Acceptable configurations} & \\\\\\\\\")\n",
    "\n",
    "print(\"  &  10\\% & 20\\% & 50\\% & 80\\% & 90\\% \\\\\\\\\")\n",
    "\n",
    "print(\"\\\\hline \\\\hline\")\n",
    "print(\"&\\\\multicolumn{5}{c|}{\\\\textbf{Classification}}&\")\n",
    "print(\"\\\\hline\")\n",
    "\n",
    "for k,i in df_classification.groupby([\"threshold\",\"training_size\"])[\"mean\"].max().unstack('threshold').iterrows():\n",
    "    s = \"{:.0f}\".format(int(k* df.shape[0]))\n",
    "    for l,j in i.iteritems():\n",
    "        j_fs = df_classification_fs.groupby([\"threshold\",\"training_size\"])[\"mean\"].max().unstack('threshold').loc[k,l]\n",
    "        diff = j_fs - j\n",
    "        j_best = j_fs if j_fs > j else j\n",
    "        best = j_best == df_all.groupby([\"threshold\",\"training_size\"])[\"mean\"].max().unstack('threshold').loc[k,l]\n",
    "        color = \"\\\\textcolor{ForestGreen}{+\" if diff > 0 else \"\\\\textcolor{red}{-\"\n",
    "        if abs(diff) <= 0.01:\n",
    "            color = color.replace(\"red\",\"gray\").replace(\"ForestGreen\",\"gray\")\n",
    "        s += \" & {}{:.1f}{} ({}{:0.1f}{})\".format(\n",
    "            \"\\\\textbf{\" if best else \"\", \n",
    "            j_best*100, \n",
    "            \"}\" if best else \"\", \n",
    "            color,\n",
    "            abs(diff*100),\n",
    "            \"}\"\n",
    "        )\n",
    "    s += \" \\\\\\\\\"\n",
    "    print(s)\n",
    "    \n",
    "print(\"\\\\hline \\\\hline\")\n",
    "print(\"&\\\\multicolumn{5}{c|}{\\\\textbf{Regression}}&\")\n",
    "print(\"\\\\hline\")\n",
    "for k,i in df_regression.groupby([\"threshold\",\"training_size\"])[\"mean\"].max().unstack('threshold').iterrows():\n",
    "    s = \"{:.0f}\".format(int(k* df.shape[0]))\n",
    "    for l,j in i.iteritems():\n",
    "        j_fs = df_regression_fs.groupby([\"threshold\",\"training_size\"])[\"mean\"].max().unstack('threshold').loc[k,l]\n",
    "        diff = j_fs - j\n",
    "        j_best = j_fs if j_fs > j else j\n",
    "        best = j_best == df_all.groupby([\"threshold\",\"training_size\"])[\"mean\"].max().unstack('threshold').loc[k,l]\n",
    "        color = \"\\\\textcolor{ForestGreen}{+\" if diff > 0 else \"\\\\textcolor{red}{-\"\n",
    "        color = color if diff >= 0.01 or diff <= -0.01 else \"\\\\textcolor{gray}{+\"\n",
    "        \n",
    "        s += \" & {}{:.1f}{} ({}{:0.1f}{})\".format(\n",
    "            \"\\\\textbf{\" if best else \"\", \n",
    "            j_best*100, \n",
    "            \"}\" if best else \"\", \n",
    "            color,\n",
    "            abs(diff*100),\n",
    "            \"}\"\n",
    "        )\n",
    "    s += \" \\\\\\\\\"\n",
    "    print(s)\n",
    "\n",
    "print(\"\\\\hline \\\\hline\")\n",
    "print(\"&\\\\multicolumn{5}{c|}{\\\\textbf{Specialized Regression}}&\")\n",
    "print(\"\\\\hline\")\n",
    "for k,i in df_spec_regression.groupby([\"threshold\",\"training_size\"])[\"mean\"].max().unstack('threshold').iterrows():\n",
    "    s = \"{:.0f}\".format(int(k* df.shape[0]))\n",
    "    for l,j in i.iteritems():\n",
    "        j_fs = df_spec_regression_fs.groupby([\"threshold\",\"training_size\"])[\"mean\"].max().unstack('threshold').loc[k,l]\n",
    "        diff = j_fs - j\n",
    "        j_best = j_fs if j_fs > j else j\n",
    "        best = j_best == df_all.groupby([\"threshold\",\"training_size\"])[\"mean\"].max().unstack('threshold').loc[k,l]\n",
    "        color = \"\\\\textcolor{ForestGreen}{+\" if diff > 0 else \"\\\\textcolor{red}{-\"\n",
    "        color = color if diff >= 0.01 or diff <= -0.01 else \"\\\\textcolor{gray}{+\"\n",
    "        s += \" & {}{:.1f}{} ({}{:0.1f}{})\".format(\n",
    "            \"\\\\textbf{\" if best else \"\", \n",
    "            j_best*100, \n",
    "            \"}\" if best else \"\", \n",
    "            color,\n",
    "            abs(diff*100),\n",
    "            \"}\"\n",
    "        )\n",
    "    s += \" \\\\\\\\\"\n",
    "    print(s)\n",
    "    \n",
    "print(\"\\\\hline\")\n",
    "print(\"\\\\end{tabular}\")\n",
    "print(\"\\\\caption{Decision tree classification accuracy on performance specialization for\", system_name, \n",
    "      \"on three strategies. Bold represents the best result among other strategies including feature selection, the value in brackets is the difference made by feature selection\\\\label{tab:\" + system_name.lower() + \"}}\")\n",
    "print(\"\\\\end{table*}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "threshold = df[\"perf\"].median()\n",
    "train_size = 0.7\n",
    "\n",
    "# Getting the best hyperparameters\n",
    "best_config_classification = df_classification.query(\"threshold == {} and training_size == {}\".format(threshold,train_size)).sort_values(\"mean\", ascending=False).iloc[0]\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop(columns=[\"perf\"]), df[\"perf\"] > threshold, train_size=train_size)\n",
    "clf = tree.DecisionTreeClassifier(max_depth=best_config_classification[\"max_depth\"], min_samples_split=best_config_classification[\"min_samples_split\"], criterion=best_config_classification[\"criterion\"])\n",
    "\n",
    "# Start timing\n",
    "time_begin = time.time()\n",
    "\n",
    "# Run the learning process\n",
    "for _ in range(0,10):\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "time_classification = (time.time() - time_begin) / 10\n",
    "\n",
    "\n",
    "\n",
    "best_config_classification_fs = df_classification_fs.query(\"threshold == {} and training_size == {}\".format(threshold,train_size)).sort_values(\"mean\", ascending=False).iloc[0]\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[feature_ranking_list[:best_config_classification_fs[\"n_options\"]]], df[\"perf\"] > threshold, train_size=train_size)\n",
    "clf = tree.DecisionTreeClassifier(max_depth=best_config_classification_fs[\"max_depth\"], min_samples_split=best_config_classification_fs[\"min_samples_split\"], criterion=best_config_classification_fs[\"criterion\"])\n",
    "\n",
    "time_begin = time.time()\n",
    "\n",
    "for _ in range(0,10):\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "time_classification_fs = (time.time() - time_begin) / 10\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "best_config_regression = df_regression.query(\"threshold == {} and training_size == {}\".format(threshold,train_size)).sort_values(\"mean\", ascending=False).iloc[0]\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop(columns=[\"perf\"]), df[\"perf\"], train_size=train_size)\n",
    "clf = tree.DecisionTreeRegressor(max_depth=best_config_regression[\"max_depth\"], min_samples_split=best_config_regression[\"min_samples_split\"], criterion=best_config_regression[\"criterion\"])\n",
    "\n",
    "time_begin = time.time()\n",
    "\n",
    "for _ in range(0,10):\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "time_regression = (time.time() - time_begin) / 10\n",
    "\n",
    "\n",
    "\n",
    "best_config_regression_fs = df_regression_fs.query(\"threshold == {} and training_size == {}\".format(threshold,train_size)).sort_values(\"mean\", ascending=False).iloc[0]\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[feature_ranking_list[:best_config_regression_fs[\"n_options\"]]], df[\"perf\"], train_size=train_size)\n",
    "clf = tree.DecisionTreeRegressor(max_depth=best_config_regression_fs[\"max_depth\"], min_samples_split=best_config_regression_fs[\"min_samples_split\"], criterion=best_config_regression_fs[\"criterion\"])\n",
    "\n",
    "time_begin = time.time()\n",
    "\n",
    "for _ in range(0,10):\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "time_regression_fs = (time.time() - time_begin) / 10\n",
    "\n",
    "\n",
    "\n",
    "best_config_spec_regression = df_spec_regression.query(\"threshold == {} and training_size == {}\".format(threshold,train_size)).sort_values(\"mean\", ascending=False).iloc[0]\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop(columns=[\"perf\"]), df[\"perf\"].map(lambda x : x + best_config_spec_regression[\"gap\"] if x > threshold else x), train_size=train_size)\n",
    "clf = tree.DecisionTreeRegressor(max_depth=best_config_spec_regression[\"max_depth\"], min_samples_split=best_config_spec_regression[\"min_samples_split\"], criterion=best_config_spec_regression[\"criterion\"])\n",
    "\n",
    "time_begin = time.time()\n",
    "\n",
    "for _ in range(0,10):\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "time_spec_regression = (time.time() - time_begin) / 10\n",
    "\n",
    "\n",
    "\n",
    "best_config_spec_regression_fs = df_spec_regression_fs.query(\"threshold == {} and training_size == {}\".format(threshold,train_size)).sort_values(\"mean\", ascending=False).iloc[0]\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[feature_ranking_list[:best_config_spec_regression_fs[\"n_options\"]]], df[\"perf\"].map(lambda x : x + best_config_spec_regression_fs[\"gap\"] if x > threshold else x), train_size=train_size)\n",
    "clf = tree.DecisionTreeRegressor(max_depth=best_config_spec_regression[\"max_depth\"], min_samples_split=best_config_spec_regression[\"min_samples_split\"], criterion=best_config_spec_regression[\"criterion\"])\n",
    "\n",
    "time_begin = time.time()\n",
    "\n",
    "for _ in range(0,10):\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "time_spec_regression_fs = (time.time() - time_begin) / 10\n",
    "\n",
    "print(\"Training time classification : \", time_classification)\n",
    "print(\"Training time classification FS : \", time_classification_fs)\n",
    "\n",
    "print(\"Training time regression : \", time_regression)\n",
    "print(\"Training time regression FS : \", time_regression_fs)\n",
    "\n",
    "print(\"Training time spec regression : \", time_spec_regression)\n",
    "print(\"Training time spec regression FS : \", time_spec_regression_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8",
   "language": "python",
   "name": "python38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
